{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c765441c",
   "metadata": {},
   "source": [
    "# Data Quality Control\n",
    "\n",
    "The datasets consist of 105 columns (103 numerical and 2 categorical) with 1 column for target (binary)\n",
    "\n",
    "Our job is to find the best classification methods to classified the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cda76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Open the data\n",
    "data  = pd.read_csv('DM_Project_24.csv')\n",
    "\n",
    "# Statistical information \n",
    "print(data.describe()) \n",
    "#Lot of missing data found because total count only ~1400-1500\n",
    "\n",
    "# See data in nicer way \n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d56e9f",
   "metadata": {},
   "source": [
    "# Data Quality Control \n",
    "## Removal of Data\n",
    "- See if we can remove NA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85544883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of NA\n",
    "df_deleting = data.copy()\n",
    "df_deleting = df_deleting.dropna()\n",
    "\n",
    "print(df_deleting.info())\n",
    "#571 left after the removal - which is not good \n",
    "\n",
    "df_deleting.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21597da8",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "We separate the numerical and categorical columns since we want approach them differently\n",
    "- Numerical: using mean \n",
    "- Categorical: using mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputation for all class ### \n",
    "\n",
    "# Separate features and target\n",
    "X_train = data.iloc[:, :-1]  # Features: columns 0-105\n",
    "y_train = data.iloc[:, -1]   # Target: column 106\n",
    "\n",
    "# Columns: numerical and categorical\n",
    "numerical_cols = X_train.columns[:-2]  # First 103 columns\n",
    "categorical_cols = (X_train.columns[-2:]) # Last 2 columns\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_imputer = SimpleImputer(strategy = \"mean\")\n",
    "X_num = pd.DataFrame(\n",
    "    numerical_imputer.fit_transform(X_train[numerical_cols]),\n",
    "    columns=numerical_cols)\n",
    "\n",
    "# Categorical Data Imputation\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_cat_imputed = pd.DataFrame(\n",
    "    categorical_imputer.fit_transform(X_train[categorical_cols]),\n",
    "    columns=categorical_cols)\n",
    "\n",
    "# Combine the categories\n",
    "X_processed = pd.concat([X_num, X_cat_imputed], axis=1)\n",
    "\n",
    "print(X_processed.head())\n",
    "\n",
    "#Cross-validation\n",
    "#Calculate F1\n",
    "cv_impu = cross_val_score(rf, X_processed, y_train, cv=KFold(n_splits=5), scoring= \"f1\")\n",
    "print(f'F1 score across folds: {cv_impu.mean():.4f}')\n",
    "#F1 = 0.7325 for imputation \n",
    "\n",
    "#Calculate accuracy \n",
    "cv_impu_acc = cross_val_score(rf, X_processed, y_train, cv=KFold(n_splits=5), scoring= \"accuracy\")\n",
    "print(f'Accuracy score across folds: {cv_impu_acc.mean():.4f}')\n",
    "#accuracy is 0.9456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputation for specific-class value ### \n",
    "# Separate features and target\n",
    "X_train = data.iloc[:, :-1]  # Features: columns 0-105\n",
    "y_train = data.iloc[:, -1]   # Target: column 106\n",
    "\n",
    "# Columns: numerical and categorical\n",
    "numerical_cols = X_train.columns[:-2].tolist()  # First 103 columns\n",
    "categorical_cols = X_train.columns[-2:].tolist() # Last 2 columns\n",
    "\n",
    "# Separate the data based on class\n",
    "class_groups = data.groupby(y_train)\n",
    "\n",
    "# Doing class specific imputation \n",
    "imputed_data = []\n",
    "for label, group in class_groups:\n",
    "    #Imputation for numerical \n",
    "    numerical_imputer = SimpleImputer(strategy = \"mean\")\n",
    "    group[numerical_cols] = numerical_imputer.fit_transform(group[numerical_cols])\n",
    "\n",
    "    #Imputation for categorical \n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    group[categorical_cols] = categorical_imputer.fit_transform(group[categorical_cols])\n",
    "    \n",
    "    imputed_data.append(group)\n",
    "\n",
    "#Cross-validation\n",
    "X_imputed_class = imputed_data.iloc[:, :-1]  # Features: columns 0-105\n",
    "y_imputed_class = imputed_data.iloc[:, -1]\n",
    "\n",
    "#Calculate F1\n",
    "cv_impu = cross_val_score(rf, X_imputed_class, y_imputed_class, cv=KFold(n_splits=5), scoring= \"f1\")\n",
    "print(f'F1 score across folds: {cv_impu.mean():.4f}')\n",
    "#F1 = 0.8217 for imputation \n",
    "\n",
    "#Calculate accuracy \n",
    "cv_impu_acc = cross_val_score(rf, X_imputed_class, y_imputed_class, cv=KFold(n_splits=5), scoring= \"accuracy\")\n",
    "print(f'Accuracy score across folds: {cv_impu_acc.mean():.4f}')\n",
    "#accuracy is 0.9621\n",
    "\n",
    "# Since the highest CV is using class-specific imputation, we used that dataset\n",
    "# for future analysis \n",
    "\n",
    "##  Import it as dataframe ##\n",
    "X_imputed_class = pd.concat(imputed_data).sort_index()\n",
    "print(X_imputed_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3382118",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ea9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all class imputation\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "#Normalize the numerical \n",
    "x_num_scaled = standard_scaler.fit_transform(X_num)\n",
    "X_num_df = pd.DataFrame(x_num_scaled, columns=numerical_cols)\n",
    "\n",
    "#Combination\n",
    "X_processed_standard = pd.concat([X_num_df, X_cat_imputed], axis=1)\n",
    "print(X_processed_standard)\n",
    "\n",
    "#Cross validation \n",
    "#Using F1\n",
    "cv_standard_all = cross_val_score(rf, X_processed_standard, y_train, cv=KFold(n_splits=5), scoring= \"f1\")\n",
    "print(f'F1 score across folds: {cv_standard_all.mean():.4f}')\n",
    "#F1 = 0.7325  \n",
    "\n",
    "#Using accuracy \n",
    "cv_standard_acc = cross_val_score(rf, X_processed_standard, y_train, cv=KFold(n_splits=5), scoring= \"accuracy\")\n",
    "print(f'Accuracy score across folds: {cv_standard_acc.mean():.4f}')\n",
    "#accuracy is 0.9456\n",
    "\n",
    "### Normalisation ###\n",
    "# For class-specific imputation\n",
    "\n",
    "#separate the numerical and categorical\n",
    "X = X_imputed_class.iloc[:, :-1]  # Features: columns 0-105\n",
    "y = X_imputed_class.iloc[:, -1]   # Target: column 106\n",
    "\n",
    "# Columns: numerical and categorical\n",
    "num_x_train = X.columns[:-2]  # First 103 columns\n",
    "nom_x_train = X.columns[-2:] # Last 2 columns\n",
    "\n",
    "# Separate numerical and categorical data\n",
    "X_num = X[num_x_train]\n",
    "X_cat = X[nom_x_train]\n",
    "\n",
    "# Ensure numerical columns are actually numeric\n",
    "X_num = X_num.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "## Standardization (Z-score normalization) ## \n",
    "standard_scaler = StandardScaler()\n",
    "X_num_standardized = pd.DataFrame(\n",
    "    standard_scaler.fit_transform(X_num),\n",
    "    columns=X_num.columns,\n",
    "    index=X_num.index)\n",
    "\n",
    "#Combine the data \n",
    "X_standardized = pd.concat([X_num_standardized, X_cat], axis=1)\n",
    "\n",
    "#Class specific imputation + z-score normalisation \n",
    "#Calculate F1\n",
    "cv_standard_class = cross_val_score(rf, X_standardized, y_train, cv=KFold(n_splits=5), scoring= \"f1\")\n",
    "print(f'F1 score across folds: {cv_standard_class.mean():.4f}')\n",
    "#F1 = 0.7897 \n",
    "\n",
    "#Calculate accuracy \n",
    "cv_acc_standard_c = cross_val_score(rf, X_standardized, y_train, cv=KFold(n_splits=5), scoring= \"accuracy\")\n",
    "print(f'Accuracy score across folds: {cv_acc_standard_c.mean():.4f}')\n",
    "#accuracy is 0.9556\n",
    "\n",
    "##  Min-Max scaling ##\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_num_minmax = pd.DataFrame(\n",
    "    minmax_scaler.fit_transform(X_num),\n",
    "    columns=X_num.columns,\n",
    "    index=X_num.index)\n",
    "#combine the data \n",
    "X_minmax = pd.concat([X_num_minmax, X_cat], axis=1)\n",
    "\n",
    "# Calculate F1\n",
    "cv_minmax_class = cross_val_score(rf, X_minmax, y_train, cv=KFold(n_splits=5), scoring= \"f1\")\n",
    "print(f'F1 score across folds: {cv_minmax_class.mean():.4f}')\n",
    "#F1 = 0.7897 \n",
    "\n",
    "#Calculate accuracy \n",
    "cv_acc_minmax_c = cross_val_score(rf, X_minmax, y_train, cv=KFold(n_splits=5), scoring= \"accuracy\")\n",
    "print(f'Accuracy score across folds: {cv_acc_minmax_c.mean():.4f}')\n",
    "#accuracy is 0.9556\n",
    "\n",
    "# They have the same F1 score, so we will proceed with class-specific imputation and z-score normalisation. \n",
    "\n",
    "# Combine scaled numerical data with categorical data\n",
    "X_standardized = pd.concat([X_num_standardized, X_cat], axis=1)\n",
    "\n",
    "print(X_standardized)\n",
    "\n",
    "### Normalisation for test data ### \n",
    "\n",
    "# Import the data\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "print(test_data)\n",
    "\n",
    "# Columns: numerical and categorical\n",
    "num_x_train = test_data.columns[:-2]  # First 103 columns\n",
    "nom_x_train = test_data.columns[-2:] # Last 2 columns\n",
    "\n",
    "# Separate numerical and ategorical data\n",
    "X_num = test_data[num_x_train]\n",
    "X_cat = test_data[nom_x_train]\n",
    "\n",
    "# Ensure numerical columns are actually numeric\n",
    "X_num = X_num.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Standardization (Z-score normalization)\n",
    "standard_scaler = StandardScaler()\n",
    "X_num_standardized = pd.DataFrame(\n",
    "    standard_scaler.fit_transform(X_num),\n",
    "    columns=X_num.columns,\n",
    "    index=X_num.index)\n",
    "\n",
    "# Import it to the data\n",
    "test_standardized = pd.concat([X_num_standardized, X_cat], axis=1)\n",
    "test_standardized.to_csv('test_standard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5c1da",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Extra libraries \n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#X_standardized = all the x\n",
    "#y_train = as y\n",
    "\n",
    "#)utliers detection using random forest \n",
    "IF = IsolationForest(random_state=0, contamination = 0.01)\n",
    "outliers = IF.fit_predict(X_standardized)\n",
    "mask = outliers == 1\n",
    "x_clean = X_standardized[mask]\n",
    "y_clean = y_train[mask]\n",
    "\n",
    "print(y_clean)\n",
    "\n",
    "#Cross validation \n",
    "#accuracy\n",
    "cv_if_acc = cross_val_score(rf, x_clean, y_clean, cv=KFold(n_splits=5), scoring=\"accuracy\")\n",
    "print(f'Accuracy score across folds: {cv_if_acc.mean():.4f}')\n",
    "#accuracy = 0.9583\n",
    "\n",
    "#f1\n",
    "cv_if = cross_val_score(rf, x_clean, y_clean, cv=KFold(n_splits=5), scoring= \"f1\")\n",
    "print(f'F1 score across folds: {cv_if.mean():.4f}')\n",
    "#F1 = 0.8102 \n",
    "\n",
    "# Removal of outliers using LOF \n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)  # Adjust neighbors and contamination\n",
    "outliers = lof.fit_predict(X_standardized)\n",
    "mask = outliers == 1\n",
    "x_clean_lof = X_standardized[mask]\n",
    "y_clean_lof = y_train[mask]\n",
    "\n",
    "#cross validation \n",
    "#accuracy\n",
    "cv_if_acc = cross_val_score(rf, x_clean_lof, y_clean_lof, cv=KFold(n_splits=5), scoring=\"accuracy\")\n",
    "print(f'Accuracy score across folds: {cv_if_acc.mean():.4f}')\n",
    "#accuracy = 0.9621\n",
    "\n",
    "#f1\n",
    "cv_if = cross_val_score(rf, x_clean_lof, y_clean_lof, cv=KFold(n_splits=5), scoring= \"f1\")\n",
    "print(f'F1 score across folds: {cv_if.mean():.4f}')\n",
    "#F1 = 0.8217 \n",
    "\n",
    "# LOF has higher F1 so we will choose this method. \n",
    "\n",
    "## Combining the data for clean datasets ##\n",
    "clean_data = pd.concat([x_clean_lof, y_clean_lof], axis=1)\n",
    "print(clean_data)\n",
    "clean_data.to_csv('train_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
